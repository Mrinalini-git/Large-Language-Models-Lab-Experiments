{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8TSnKbZRKoM",
        "outputId": "086dea1d-32c8-43f0-9b04-1b18587dda22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input sequence (x):\n",
            "torch.Size([1, 5, 8])\n",
            "\n",
            "--- Single-Head Attention Demonstration ---\n",
            "Single-Head Attention Output Shape: torch.Size([1, 5, 8])\n",
            "Single-Head Attention Weights Shape: torch.Size([1, 5, 5])\n",
            "\n",
            "--- Multi-Head Attention Demonstration ---\n",
            "Multi-Head Attention Output Shape: torch.Size([1, 5, 8])\n",
            "Multi-Head Attention Weights Shape: torch.Size([1, 2, 5, 5]) (batch, heads, seq_len, seq_len)\n",
            "\n",
            "Key Differences:\n",
            "- Single-Head Attention: Computes one set of attention weights and output vectors for the entire input dimension.\n",
            "- Multi-Head Attention: Divides the input into 'num_heads' smaller parts (or projects into 'num_heads' different subspaces), computes attention independently for each head, and then concatenates and linearly transforms the results. This allows the model to jointly attend to information from different representation subspaces at different positions.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "# --- 1. Synthetic Dataset --- #\n",
        "# Let's create a simple sequence of embeddings (e.g., word embeddings for a short sentence)\n",
        "# batch_size = 1, sequence_length = 5, embedding_dimension = 8\n",
        "embedding_dim = 8\n",
        "sequence_length = 5\n",
        "batch_size = 1\n",
        "x = torch.randn(batch_size, sequence_length, embedding_dim)\n",
        "print(f\"Input sequence (x):\\n{x.shape}\\n\")\n",
        "\n",
        "# --- 2. Single-Head Attention Implementation --- #\n",
        "class SingleHeadAttention(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.query_proj = nn.Linear(input_dim, output_dim)\n",
        "        self.key_proj = nn.Linear(input_dim, output_dim)\n",
        "        self.value_proj = nn.Linear(input_dim, output_dim)\n",
        "        self.scale = math.sqrt(output_dim) # Scaling factor\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        # Project inputs to query, key, value spaces\n",
        "        Q = self.query_proj(query) # (batch_size, seq_len, output_dim)\n",
        "        K = self.key_proj(key)     # (batch_size, seq_len, output_dim)\n",
        "        V = self.value_proj(value) # (batch_size, seq_len, output_dim)\n",
        "\n",
        "        # Calculate attention scores (dot product attention)\n",
        "        # Q @ K.transpose(-2, -1) -> (batch_size, seq_len, seq_len)\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9) # Apply mask (e.g., for padding or causality)\n",
        "\n",
        "        attention_weights = torch.softmax(scores, dim=-1)\n",
        "        attention_weights = self.dropout(attention_weights)\n",
        "\n",
        "        # Multiply with Value matrix\n",
        "        output = torch.matmul(attention_weights, V) # (batch_size, seq_len, output_dim)\n",
        "        return output, attention_weights\n",
        "\n",
        "print(\"--- Single-Head Attention Demonstration ---\")\n",
        "# For single-head attention, output_dim can be same as embedding_dim\n",
        "single_head_attn = SingleHeadAttention(embedding_dim, embedding_dim)\n",
        "single_output, single_weights = single_head_attn(x, x, x) # Q, K, V are all from x (self-attention)\n",
        "\n",
        "print(f\"Single-Head Attention Output Shape: {single_output.shape}\")\n",
        "print(f\"Single-Head Attention Weights Shape: {single_weights.shape}\\n\")\n",
        "# print(f\"Sample Single-Head Attention Output:\\n{single_output[0,0,:4].detach().numpy()}\\n\")\n",
        "# print(f\"Sample Single-Head Attention Weights for first token:\\n{single_weights[0,0,:].detach().numpy()}\\n\")\n",
        "\n",
        "\n",
        "# --- 3. Multi-Head Attention Implementation --- #\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, input_dim, head_dim, num_heads, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = head_dim\n",
        "        self.input_dim = input_dim\n",
        "\n",
        "        # All projections are combined into one for efficiency\n",
        "        self.query_proj = nn.Linear(input_dim, num_heads * head_dim)\n",
        "        self.key_proj = nn.Linear(input_dim, num_heads * head_dim)\n",
        "        self.value_proj = nn.Linear(input_dim, num_heads * head_dim)\n",
        "        self.output_proj = nn.Linear(num_heads * head_dim, input_dim) # Final linear layer\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.scale = math.sqrt(head_dim)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size, seq_len, _ = query.shape\n",
        "\n",
        "        # Project inputs and split into heads\n",
        "        Q = self.query_proj(query).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        K = self.key_proj(key).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.value_proj(value).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Q, K, V now have shape (batch_size, num_heads, seq_len, head_dim)\n",
        "\n",
        "        # Calculate attention scores for all heads in parallel\n",
        "        # (batch_size, num_heads, seq_len, head_dim) @ (batch_size, num_heads, head_dim, seq_len)\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale # (batch_size, num_heads, seq_len, seq_len)\n",
        "\n",
        "        if mask is not None:\n",
        "            # Ensure mask is broadcastable if it's 2D (seq_len, seq_len)\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        attention_weights = torch.softmax(scores, dim=-1)\n",
        "        attention_weights = self.dropout(attention_weights)\n",
        "\n",
        "        # Apply attention to Value\n",
        "        # (batch_size, num_heads, seq_len, seq_len) @ (batch_size, num_heads, seq_len, head_dim)\n",
        "        x = torch.matmul(attention_weights, V) # (batch_size, num_heads, seq_len, head_dim)\n",
        "\n",
        "        # Concatenate heads and apply final linear projection\n",
        "        x = x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.num_heads * self.head_dim)\n",
        "        output = self.output_proj(x)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "print(\"--- Multi-Head Attention Demonstration ---\")\n",
        "num_heads = 2\n",
        "head_dim = embedding_dim // num_heads # Each head processes a part of the embedding dimension\n",
        "\n",
        "multi_head_attn = MultiHeadAttention(embedding_dim, head_dim, num_heads)\n",
        "multi_output, multi_weights = multi_head_attn(x, x, x) # Self-attention\n",
        "\n",
        "print(f\"Multi-Head Attention Output Shape: {multi_output.shape}\")\n",
        "print(f\"Multi-Head Attention Weights Shape: {multi_weights.shape} (batch, heads, seq_len, seq_len)\\n\")\n",
        "# print(f\"Sample Multi-Head Attention Output:\\n{multi_output[0,0,:4].detach().numpy()}\\n\")\n",
        "# print(f\"Sample Multi-Head Attention Weights for first token (head 0):\\n{multi_weights[0,0,0,:].detach().numpy()}\\n\")\n",
        "\n",
        "print(\"Key Differences:\")\n",
        "print(\"- Single-Head Attention: Computes one set of attention weights and output vectors for the entire input dimension.\")\n",
        "print(\"- Multi-Head Attention: Divides the input into 'num_heads' smaller parts (or projects into 'num_heads' different subspaces), computes attention independently for each head, and then concatenates and linearly transforms the results. This allows the model to jointly attend to information from different representation subspaces at different positions.\")\n"
      ]
    }
  ]
}