{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5975c475",
        "outputId": "6565bf99-534f-421b-c31b-388cf5942f34"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "try:\n",
        "    from transformers import AutoTokenizer, AutoModel\n",
        "    import torch\n",
        "except ImportError:\n",
        "    print(\"Hugging Face Transformers and PyTorch not installed. Please install with: pip install transformers torch\")\n",
        "\n",
        "try:\n",
        "    import gensim.downloader as api\n",
        "    from gensim.models import Word2Vec\n",
        "except ImportError:\n",
        "    print(\"Gensim not installed. Please install with: pip install gensim\")\n",
        "\n",
        "# Download NLTK data if not already present\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    print(\"Downloading NLTK 'punkt' tokenizer...\")\n",
        "    nltk.download('punkt')\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    print(\"Downloading NLTK 'stopwords' corpus...\")\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "# Define variables for examples\n",
        "text = 'Hello, world! This is an example sentence for NLTK tokenization.'\n",
        "sentences = ['The cat sat on the mat.', 'The dog ran in the park.', 'A cat and a dog are pets.', 'Cats and dogs are common household animals.']\n",
        "\n",
        "print(\"Variables 'text' and 'sentences' initialized.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variables 'text' and 'sentences' initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a268a31f"
      },
      "source": [
        "### 1. Whitespace Tokenization\n",
        "This is the simplest form of tokenization, where text is split by whitespace characters. It's fast but can be less accurate as it doesn't handle punctuation attached to words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7656e03b"
      },
      "source": [
        "### 2. Character Tokenization\n",
        "This method breaks down text into individual characters. It's very granular but often results in a large number of tokens, which can make analysis complex and computationally intensive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f39f9700"
      },
      "source": [
        "### 3. Subword Tokenization (using BERT tokenizer)\n",
        "Subword tokenization splits words into smaller units (subwords or morphemes). This approach helps to handle out-of-vocabulary words and reduces vocabulary size while still capturing semantic meaning. BERT's WordPiece tokenizer is a common example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c26a6cf0"
      },
      "source": [
        "### 4. Tokenization with Stop Word Removal\n",
        "This technique involves first tokenizing the text and then removing common words (stop words) that carry little semantic meaning (e.g., 'the', 'is', 'a'). This can reduce noise and improve the efficiency of text analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b37a5bf5"
      },
      "source": [
        "### 5. Sentence Tokenization\n",
        "Sentence tokenization divides a text into a sequence of sentences. This is crucial for tasks that require understanding text at a sentence level, such as sentiment analysis or machine translation, and NLTK's `sent_tokenize` is widely used for this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d5d5849"
      },
      "source": [
        "### 6. Word Tokenization (using NLTK)\n",
        "Word tokenization breaks a text into individual words or punctuation marks. NLTK's `word_tokenize` is a robust method that typically separates punctuation from words, offering a more refined tokenization than simple whitespace splitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ada38dea"
      },
      "source": [
        "### 1. One-Hot Encoding\n",
        "One-Hot Encoding represents words as sparse binary vectors, where each word is assigned a unique index, and its vector has a '1' at that index and '0's elsewhere. It's simple but leads to high-dimensional vectors for large vocabularies and doesn't capture semantic relationships between words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04dbbef2"
      },
      "source": [
        "### 2. Bag-of-Words (BoW)\n",
        "Bag-of-Words represents a document as an unordered collection of words, disregarding grammar and word order. It focuses on the frequency of words in a document. While simple and effective for some tasks, it loses contextual information and can result in high-dimensional sparse vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "708cda28"
      },
      "source": [
        "### 3. Word2Vec\n",
        "Word2Vec is a predictive model that learns to represent words as dense, continuous vectors (embeddings) in a lower-dimensional space. Words with similar meanings are located closer together in this vector space, capturing semantic relationships. It uses either a Continuous Bag-of-Words (CBOW) or Skip-gram architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4763249b"
      },
      "source": [
        "### 4. GloVe (Conceptual / using gensim.downloader for pre-trained)\n",
        "Global Vectors for Word Representation (GloVe) is an unsupervised learning algorithm for obtaining vector representations for words. It captures global co-occurrence statistics of words in a corpus. Like Word2Vec, it generates dense embeddings where semantic similarities are reflected by vector proximity. Pre-trained GloVe models are often used for efficiency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e936e32"
      },
      "source": [
        "### 5. BERT Embeddings\n",
        "BERT (Bidirectional Encoder Representations from Transformers) provides contextualized word embeddings. Unlike Word2Vec or GloVe, BERT generates embeddings for a word that can vary based on its context in a sentence, making it highly effective for understanding nuanced meanings. It's a deep neural network model trained on a large corpus of text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f80b1540"
      },
      "source": [
        "### 1. Whitespace Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a7a1ff6",
        "outputId": "f1d70cba-8622-4e7b-915b-eb159046e054"
      },
      "source": [
        "print(\"1. Whitespace Tokenization:\")\n",
        "whitespace_tokens = text.split()\n",
        "print(f\"Original text: '{text}'\")\n",
        "print(f\"Whitespace tokens: {whitespace_tokens}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Whitespace Tokenization:\n",
            "Original text: 'Hello, world! This is an example sentence for NLTK tokenization.'\n",
            "Whitespace tokens: ['Hello,', 'world!', 'This', 'is', 'an', 'example', 'sentence', 'for', 'NLTK', 'tokenization.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8a70a8e"
      },
      "source": [
        "### 2. Character Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9affbe5",
        "outputId": "8f652de8-cba9-4beb-b97c-617b3fe34f40"
      },
      "source": [
        "print(\"2. Character Tokenization:\")\n",
        "char_tokens = list(text)\n",
        "print(f\"Original text: '{text}'\")\n",
        "print(f\"Character tokens: {char_tokens}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2. Character Tokenization:\n",
            "Original text: 'Hello, world! This is an example sentence for NLTK tokenization.'\n",
            "Character tokens: ['H', 'e', 'l', 'l', 'o', ',', ' ', 'w', 'o', 'r', 'l', 'd', '!', ' ', 'T', 'h', 'i', 's', ' ', 'i', 's', ' ', 'a', 'n', ' ', 'e', 'x', 'a', 'm', 'p', 'l', 'e', ' ', 's', 'e', 'n', 't', 'e', 'n', 'c', 'e', ' ', 'f', 'o', 'r', ' ', 'N', 'L', 'T', 'K', ' ', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8101ea63"
      },
      "source": [
        "### 3. Subword Tokenization (using BERT tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80fce29e",
        "outputId": "00df9442-0858-477b-aa22-44955b8f6686"
      },
      "source": [
        "try:\n",
        "    from transformers import AutoTokenizer\n",
        "    tokenizer_bert_subword = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    subword_tokens = tokenizer_bert_subword.tokenize(text)\n",
        "    print(f\"Original text: '{text}'\")\n",
        "    print(f\"Subword tokens: {subword_tokens}\")\n",
        "except (ImportError, Exception) as e:\n",
        "    print(f\"Could not perform subword tokenization: {e}\")\n",
        "    print(\"Skipping subword tokenization. Ensure 'transformers' library is installed.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text: 'Hello, world! This is an example sentence for NLTK tokenization.'\n",
            "Subword tokens: ['hello', ',', 'world', '!', 'this', 'is', 'an', 'example', 'sentence', 'for', 'nl', '##t', '##k', 'token', '##ization', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2ed09be"
      },
      "source": [
        "### 4. Tokenization with Stop Word Removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5f8f9a6",
        "outputId": "e5626a30-54e0-42e8-858a-5ef8562ed2fa"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "print(\"4. Tokenization with Stop Word Removal:\")\n",
        "stop_words = set(stopwords.words('english'))\n",
        "word_tokens_with_stopwords = nltk.word_tokenize(text.lower())\n",
        "filtered_tokens = [word for word in word_tokens_with_stopwords if word.isalnum() and word not in stop_words]\n",
        "print(f\"Original text: '{text}'\")\n",
        "print(f\"Tokens after stop word removal: {filtered_tokens}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4. Tokenization with Stop Word Removal:\n",
            "Original text: 'Hello, world! This is an example sentence for NLTK tokenization.'\n",
            "Tokens after stop word removal: ['hello', 'world', 'example', 'sentence', 'nltk', 'tokenization']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b410250"
      },
      "source": [
        "### 5. Sentence Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9ba6f59",
        "outputId": "6afc936b-7c6f-40a2-f790-b5fed61d4854"
      },
      "source": [
        "print(\"5. Sentence Tokenization:\")\n",
        "sentence_tokens = nltk.sent_tokenize(text) # Using the `text` variable for a single sentence example\n",
        "# For the `sentences` list:\n",
        "all_sentences_tokenized = []\n",
        "for s in sentences:\n",
        "    all_sentences_tokenized.extend(nltk.sent_tokenize(s))\n",
        "print(f\"Original text: '{text}'\")\n",
        "print(f\"Sentence tokens (from 'text'): {sentence_tokens}\")\n",
        "print(f\"Sentence tokens (from 'sentences' list): {all_sentences_tokenized}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5. Sentence Tokenization:\n",
            "Original text: 'Hello, world! This is an example sentence for NLTK tokenization.'\n",
            "Sentence tokens (from 'text'): ['Hello, world!', 'This is an example sentence for NLTK tokenization.']\n",
            "Sentence tokens (from 'sentences' list): ['The cat sat on the mat.', 'The dog ran in the park.', 'A cat and a dog are pets.', 'Cats and dogs are common household animals.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6190a07"
      },
      "source": [
        "### 6. Word Tokenization (using NLTK)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48506dcf",
        "outputId": "681e6ab0-4304-432d-f856-f602c1b6fd91"
      },
      "source": [
        "print(\"6. Word Tokenization (using NLTK):\")\n",
        "word_tokens_nltk = nltk.word_tokenize(text)\n",
        "print(f\"Original text: '{text}'\")\n",
        "print(f\"NLTK Word tokens: {word_tokens_nltk}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6. Word Tokenization (using NLTK):\n",
            "Original text: 'Hello, world! This is an example sentence for NLTK tokenization.'\n",
            "NLTK Word tokens: ['Hello', ',', 'world', '!', 'This', 'is', 'an', 'example', 'sentence', 'for', 'NLTK', 'tokenization', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "849e6eb3"
      },
      "source": [
        "--- Embedding Examples ---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "608a64e2",
        "outputId": "6bfaae81-2f52-432b-fc8e-ab787ce3f261"
      },
      "source": [
        "# Sample corpus for embeddings (using the `sentences` variable)\n",
        "corpus = sentences\n",
        "print(f\"Corpus for embeddings: {corpus}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus for embeddings: ['The cat sat on the mat.', 'The dog ran in the park.', 'A cat and a dog are pets.', 'Cats and dogs are common household animals.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a71039f5"
      },
      "source": [
        "### 1. One-Hot Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bfe0518",
        "outputId": "88b7fafc-40d6-424c-aa63-3eddca5a00bb"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "print(\"1. One-Hot Encoding:\")\n",
        "# First, tokenize the corpus to get unique words\n",
        "all_words = []\n",
        "for s in corpus:\n",
        "    all_words.extend(nltk.word_tokenize(s.lower()))\n",
        "unique_words = sorted(list(set(all_words)))\n",
        "\n",
        "# Example for a single word using a manual mapping\n",
        "word_for_onehot = 'cat'\n",
        "if word_for_onehot in unique_words:\n",
        "    one_hot_vector = np.zeros(len(unique_words))\n",
        "    one_hot_index = unique_words.index(word_for_onehot)\n",
        "    one_hot_vector[one_hot_index] = 1\n",
        "    print(f\"Manual one-hot encoding for '{word_for_onehot}': {one_hot_vector}\")\n",
        "else:\n",
        "    print(f\"'{word_for_onehot}' not in vocabulary for one-hot encoding.\")\n",
        "\n",
        "# Using sklearn OneHotEncoder for a more robust example\n",
        "words_for_encoding = np.array(unique_words).reshape(-1, 1)\n",
        "encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "encoder.fit(words_for_encoding)\n",
        "\n",
        "encoded_word_example = np.array([['dog']]).reshape(-1, 1)\n",
        "if 'dog' in unique_words:\n",
        "    encoded_vector = encoder.transform(encoded_word_example)\n",
        "    print(f\"One-hot encoding for 'dog' (using sklearn): {encoded_vector[0]}\")\n",
        "else:\n",
        "    print(\"'dog' not in vocabulary for sklearn one-hot encoding.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. One-Hot Encoding:\n",
            "Manual one-hot encoding for 'cat': [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "One-hot encoding for 'dog' (using sklearn): [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7326415"
      },
      "source": [
        "### 2. Bag-of-Words (BoW)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a2354e3",
        "outputId": "692a5787-ccf9-4049-e4ef-acd1f60ec0d1"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "print(\"2. Bag-of-Words (BoW):\")\n",
        "vectorizer = CountVectorizer()\n",
        "bow_matrix = vectorizer.fit_transform(corpus)\n",
        "print(f\"Vocabulary: {vectorizer.get_feature_names_out()}\")\n",
        "print(f\"Bag-of-Words matrix (shape {bow_matrix.shape}):\\n{bow_matrix.toarray()}\")\n",
        "print(f\"Example sentence BoW for '{corpus[0]}': {bow_matrix.toarray()[0]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2. Bag-of-Words (BoW):\n",
            "Vocabulary: ['and' 'animals' 'are' 'cat' 'cats' 'common' 'dog' 'dogs' 'household' 'in'\n",
            " 'mat' 'on' 'park' 'pets' 'ran' 'sat' 'the']\n",
            "Bag-of-Words matrix (shape (4, 17)):\n",
            "[[0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1 2]\n",
            " [0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 2]\n",
            " [1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 0 0]\n",
            " [1 1 1 0 1 1 0 1 1 0 0 0 0 0 0 0 0]]\n",
            "Example sentence BoW for 'The cat sat on the mat.': [0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77564b4e"
      },
      "source": [
        "### 3. Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dfe57f6",
        "outputId": "8ca3c846-9dfc-4c41-afe9-38c7cfc79477"
      },
      "source": [
        "try:\n",
        "    from gensim.models import Word2Vec\n",
        "    print(\"3. Word2Vec:\")\n",
        "    tokenized_corpus_w2v = [nltk.word_tokenize(s.lower()) for s in corpus]\n",
        "\n",
        "    # Train a simple Word2Vec model\n",
        "    model_w2v = Word2Vec(tokenized_corpus_w2v, vector_size=100, window=5, min_count=1, workers=4)\n",
        "    model_w2v.train(tokenized_corpus_w2v, total_examples=len(tokenized_corpus_w2v), epochs=10)\n",
        "\n",
        "    word_for_w2v = 'cat'\n",
        "    if word_for_w2v in model_w2v.wv:\n",
        "        print(f\"Word2Vec embedding for '{word_for_w2v}':\\n{model_w2v.wv[word_for_w2v][:10]}...\") # print first 10 dimensions\n",
        "    else:\n",
        "        print(f\"'{word_for_w2v}' not in Word2Vec model vocabulary.\")\n",
        "\n",
        "    word_similarities = model_w2v.wv.most_similar('cat', topn=3)\n",
        "    print(f\"Words most similar to 'cat': {word_similarities}\")\n",
        "\n",
        "except (ImportError, Exception) as e:\n",
        "    print(f\"Could not train Word2Vec model: {e}\")\n",
        "    print(\"Skipping Word2Vec example. Ensure 'gensim' library is installed.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3. Word2Vec:\n",
            "Word2Vec embedding for 'cat':\n",
            "[ 8.1322715e-03 -4.4573341e-03 -1.0683573e-03  1.0063648e-03\n",
            " -1.9111396e-04  1.1481774e-03  6.1138608e-03 -2.0271540e-05\n",
            " -3.2459653e-03 -1.5107286e-03]...\n",
            "Words most similar to 'cat': [('mat', 0.1637783795595169), ('are', 0.1460077166557312), ('sat', 0.07480262219905853)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bbf5feb"
      },
      "source": [
        "### 4. GloVe (Conceptual / using gensim.downloader for pre-trained)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da9046f8",
        "outputId": "cd6560a8-f943-435d-da27-1d8c1e448448"
      },
      "source": [
        "try:\n",
        "    import gensim.downloader as api\n",
        "    print(\"4. GloVe (Conceptual / using gensim.downloader for pre-trained):\")\n",
        "    # Leveraging existing `cat_vector` and `dog_vector` from kernel for conceptual demonstration\n",
        "    if 'cat_vector' in globals() and 'dog_vector' in globals():\n",
        "        print(f\"Using existing `cat_vector` as a conceptual GloVe-like embedding: {cat_vector[:10]}...\")\n",
        "        print(f\"Using existing `dog_vector` as a conceptual GloVe-like embedding: {dog_vector[:10]}...\")\n",
        "        print(\"GloVe embeddings are typically loaded from pre-trained files and provide similar vector representations.\")\n",
        "    else:\n",
        "        print(\"No pre-existing 'cat_vector' or 'dog_vector' found for conceptual GloVe example.\")\n",
        "        print(\"GloVe embeddings are typically loaded from pre-trained files (e.g., .txt files) or via libraries like `gensim.downloader`.\")\n",
        "        print(\"Example using `gensim.downloader` (may require `pip install smart_open` and take time to download):\")\n",
        "        try:\n",
        "            print(\"  Attempting to load small pre-trained GloVe model (glove-wiki-gigaword-50)...\")\n",
        "            glove_model = api.load(\"glove-wiki-gigaword-50\")\n",
        "            word_for_glove = 'dog'\n",
        "            if word_for_glove in glove_model:\n",
        "                print(f\"  GloVe embedding for '{word_for_glove}':\\n  {glove_model[word_for_glove][:10]}...\")\n",
        "                print(f\"  Words most similar to '{word_for_glove}': {glove_model.most_similar(word_for_glove, topn=3)}\")\n",
        "            else:\n",
        "                print(f\"  '{word_for_glove}' not in GloVe model vocabulary.\")\n",
        "        except (ImportError, Exception) as e:\n",
        "            print(f\"  Could not load GloVe model from gensim.downloader: {e}\")\n",
        "            print(\"  Skipping GloVe download. Ensure 'gensim' and 'smart_open' are installed.\")\n",
        "except ImportError:\n",
        "    print(\"Gensim not installed. Skipping GloVe example.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4. GloVe (Conceptual / using gensim.downloader for pre-trained):\n",
            "Using existing `cat_vector` as a conceptual GloVe-like embedding: [ 8.1322715e-03 -4.4573341e-03 -1.0683573e-03  1.0063648e-03\n",
            " -1.9111396e-04  1.1481774e-03  6.1138608e-03 -2.0271540e-05\n",
            " -3.2459653e-03 -1.5107286e-03]...\n",
            "Using existing `dog_vector` as a conceptual GloVe-like embedding: [-0.00872748  0.00213016 -0.00087354 -0.00931909 -0.00942814 -0.00141072\n",
            "  0.00443241  0.00370407 -0.00649869 -0.00687307]...\n",
            "GloVe embeddings are typically loaded from pre-trained files and provide similar vector representations.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7309df0"
      },
      "source": [
        "### 5. BERT Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99525188",
        "outputId": "92cf86a8-0f8f-490e-8eda-6368a1bf74b8"
      },
      "source": [
        "try:\n",
        "    from transformers import AutoTokenizer, AutoModel\n",
        "    import torch\n",
        "\n",
        "    print(\"5. BERT Embeddings:\")\n",
        "    tokenizer_bert_embed = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    model_bert_embed = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "    # Example: get embedding for a sentence\n",
        "    sentence_for_bert = corpus[0] # \"The cat sat on the mat.\"\n",
        "    inputs = tokenizer_bert_embed(sentence_for_bert, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    with torch.no_grad(): # Disable gradient calculations for inference\n",
        "        outputs = model_bert_embed(**inputs)\n",
        "\n",
        "    # The last_hidden_state contains token embeddings\n",
        "    token_embeddings = outputs.last_hidden_state\n",
        "    print(f\"BERT token embeddings shape (batch_size, sequence_length, hidden_size): {token_embeddings.shape}\")\n",
        "    print(f\"First token ('[CLS]') embedding (first 10 dimensions): {token_embeddings[0, 0, :10].numpy()}...\")\n",
        "\n",
        "    # To get a sentence embedding, often the [CLS] token embedding or average of all tokens is used\n",
        "    sentence_embedding = token_embeddings[0, 0, :].numpy() # [CLS] token embedding\n",
        "    print(f\"BERT sentence embedding (from [CLS] token, first 10 dimensions): {sentence_embedding[:10]}...\")\n",
        "\n",
        "except (ImportError, Exception) as e:\n",
        "    print(f\"Could not perform BERT embeddings: {e}\")\n",
        "    print(\"Skipping BERT embeddings example. Ensure 'transformers' and 'torch' libraries are installed.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5. BERT Embeddings:\n",
            "BERT token embeddings shape (batch_size, sequence_length, hidden_size): torch.Size([1, 9, 768])\n",
            "First token ('[CLS]') embedding (first 10 dimensions): [-0.3642237  -0.05305378 -0.36732262 -0.02967339 -0.460784   -0.10106134\n",
            "  0.01669817  0.59577715 -0.11770311  0.10289837]...\n",
            "BERT sentence embedding (from [CLS] token, first 10 dimensions): [-0.3642237  -0.05305378 -0.36732262 -0.02967339 -0.460784   -0.10106134\n",
            "  0.01669817  0.59577715 -0.11770311  0.10289837]...\n"
          ]
        }
      ]
    }
  ]
}